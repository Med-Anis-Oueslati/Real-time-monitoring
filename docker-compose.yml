services:
#FLUENTD CONTAINER
  fluentd:
    image: fluent/fluentd:v1.16
    volumes:
      - /home/anis/PFE/fluentd.conf:/fluentd/etc/fluent.conf
      - /var/log:/var/log
    ports:
      - "514:514/udp"
    command: fluentd -c /fluentd/etc/fluent.conf

#ZOOKEEPER KAFKA KAFKA-CONNECT CONTAINER
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - fluentkafka

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://10.71.0.35:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_MESSAGE_MAX_BYTES: 0485760  # 10MB
    ports:
      - "9093:9093"  # Maps EXTERNAL listener
    hostname: kafka
    networks:
      - fluentkafka
      - hdfs_network

  kafka-connect:
    build:
      context: .
      dockerfile: Dockerfile.kafka-connect
    depends_on:
      - kafka
      - namenode
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: hdfs-sink-group
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      JAVA_HOME: /usr/lib/jvm/zulu11-ca
      PATH: /usr/lib/jvm/zulu11-ca/bin:$PATH:/opt/hadoop/bin:/opt/hadoop/sbin
      HADOOP_HOME: /opt/hadoop
    ports:
      - "8083:8083"
    volumes:
      - ./kafka-connect-plugins:/usr/share/confluent-hub-components
      - ./hadoop_config:/opt/hadoop/etc/hadoop:ro  # Read-only to preserve custom config
    networks:
      - fluentkafka
      - hdfs_network
#HDFS CONTAINER
  namenode:
    image: apache/hadoop:3.3.5
    container_name: namenode
    hostname: namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.el7_6.x86_64/jre
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root  # For start-dfs.sh
      - HDFS_SECONDARYNAMENODE_USER=root  # For secondary NameNode
    volumes:
      - ./hadoop_namenode:/opt/hadoop/data/nameNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./start-hdfs.sh:/start-hdfs.sh
    ports:
      - "9870:9870"
      - "8020:8020"
    command: ["/bin/bash", "/start-hdfs.sh"]
    networks:
      hdfs_network:
        ipv4_address: 172.20.0.2

  datanode1:
    image: apache/hadoop:3.3.5
    container_name: datanode1
    hostname: datanode1
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.el7_6.x86_64/jre
      - HDFS_DATANODE_USER=root
    volumes:
      - ./hadoop_datanode1:/opt/hadoop/data/dataNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./init-datanode.sh:/init-datanode.sh
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]
    networks:
      hdfs_network:
        ipv4_address: 172.20.0.3

  datanode2:
    image: apache/hadoop:3.3.5
    container_name: datanode2
    hostname: datanode2
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.el7_6.x86_64/jre
      - HDFS_DATANODE_USER=root
    volumes:
      - ./hadoop_datanode2:/opt/hadoop/data/dataNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./init-datanode.sh:/init-datanode.sh
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]
    networks:
      hdfs_network:
        ipv4_address: 172.20.0.4

#SPARK CONTAINER
  spark:
    image: bitnami/spark:3.5.0
    build:
      context: .
      dockerfile: Dockerfile.spark
    hostname: spark
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark
      - SPARK_MASTER_PORT=7077
    ports:
      - "8080:8080"  # Spark UI
      - "7077:7077"  # Master port
    networks:
      - fluentkafka
    command: bash -c "/opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master"
    volumes:
      - ./spark-scripts:/spark-scripts
      - ./spark-jars/spark-sql-kafka-0-10_2.12-3.5.0.jar:/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar
      - ./spark-logs:/opt/spark/logs
      - ./GeoLite2-City.mmdb:/opt/spark/GeoLite2-City.mmdb  # Add GeoIP database


  spark-worker:
    image: bitnami/spark:3.5.0
    build:
      context: .
      dockerfile: Dockerfile.spark
    depends_on:
      - spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=4g
    networks:
      - fluentkafka
    volumes:
      - ./spark-scripts:/spark-scripts
      - ./spark-jars/spark-sql-kafka-0-10_2.12-3.5.0.jar:/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar
      - ./spark-logs:/opt/spark/logs
      - ./GeoLite2-City.mmdb:/opt/spark/GeoLite2-City.mmdb  # Add GeoIP database

#POSTGRES CONTAINER
  db:
    image: postgres:15
    environment:
      POSTGRES_DB: siem
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

#GRAFANA CONTAINER
  grafana:
    image: grafana/grafana:10.0.0
    ports:
      - "3000:3000"
    volumes:
      - ./grafana_data:/var/lib/grafanaÂ²

#NETWORKS
networks:

  hdfs_network:
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16

  datapipeline:
    ipam:
      driver: default
      config:
        - subnet: 172.12.0.0/16

  fluentkafka:
    driver: bridge